


# --- FILE: src/blackice/__init__.py ---

from .detector import RegimeDetector, DetectionEvent

__version__ = "1.2.0"
__all__ = ["RegimeDetector", "DetectionEvent"]


# --- FILE: src/blackice/baseline.py ---

from dataclasses import dataclass
from typing import Optional
import math


@dataclass(frozen=True)
class BaselineStats:
    mean: float
    variance: float
    std: float
    count: int
    is_warm: bool
    ewma: Optional[float] = None


class RollingBuffer:
    __slots__ = ('_buffer', '_capacity', '_head', '_size')
    
    def __init__(self, capacity: int) -> None:
        if capacity < 1:
            raise ValueError("capacity must be at least 1")
        self._buffer: list[Optional[float]] = [None] * capacity
        self._capacity = capacity
        self._head = 0
        self._size = 0
    
    def push(self, value: float) -> Optional[float]:
        displaced = None
        if self._size == self._capacity:
            displaced = self._buffer[self._head]
        else:
            self._size += 1
        
        self._buffer[self._head] = value
        self._head = (self._head + 1) % self._capacity
        return displaced
    
    @property
    def is_full(self) -> bool:
        return self._size == self._capacity
    
    @property
    def size(self) -> int:
        return self._size
    
    @property
    def capacity(self) -> int:
        return self._capacity
    
    def clear(self) -> None:
        self._buffer = [None] * self._capacity
        self._head = 0
        self._size = 0


class BaselineComputer:
    __slots__ = (
        'window_size', 'use_ewma', 'ewma_alpha', 'min_std',
        '_buffer', '_mean', '_m2', '_ewma', '_total_count'
    )
    
    def __init__(
        self,
        window_size: int,
        use_ewma: bool = False,
        ewma_alpha: float = 0.3,
        min_std: float = 1e-8
    ) -> None:
        if window_size < 2:
            raise ValueError("window_size must be at least 2")
        if not (0 < ewma_alpha <= 1):
            raise ValueError("ewma_alpha must be in (0, 1]")
        if min_std < 0:
            raise ValueError("min_std must be non-negative")
        
        self.window_size = window_size
        self.use_ewma = use_ewma
        self.ewma_alpha = ewma_alpha
        self.min_std = min_std
        
        self._buffer = RollingBuffer(window_size)
        self._mean: float = 0.0
        self._m2: float = 0.0
        self._ewma: Optional[float] = None
        self._total_count: int = 0
    
    def update(self, value: float) -> bool:
        if not math.isfinite(value):
            return False
        
        n = self._buffer.size
        displaced = self._buffer.push(value)
        self._total_count += 1
        
        if displaced is None:
            n_new = n + 1
            delta = value - self._mean
            self._mean += delta / n_new
            delta2 = value - self._mean
            self._m2 += delta * delta2
        else:
            old_mean = self._mean
            self._mean += (value - displaced) / self.window_size
            
            self._m2 += (value - displaced) * (
                (value - self._mean) + (displaced - old_mean)
            )
            self._m2 = max(0.0, self._m2)
        
        if self.use_ewma:
            if self._ewma is None:
                self._ewma = value
            else:
                self._ewma = self.ewma_alpha * value + (1 - self.ewma_alpha) * self._ewma
        
        return True
    
    @property
    def is_warm(self) -> bool:
        return self._buffer.is_full
    
    @property
    def is_ready(self) -> bool:
        return self.is_warm
    
    @property
    def count(self) -> int:
        return self._buffer.size
    
    @property
    def total_count(self) -> int:
        return self._total_count
    
    @property
    def mean(self) -> float:
        return self._mean if self._buffer.size > 0 else 0.0
    
    @property
    def variance(self) -> float:
        n = self._buffer.size
        if n < 2:
            return 0.0
        return self._m2 / n
    
    @property
    def std(self) -> float:
        return max(math.sqrt(self.variance), self.min_std)
    
    @property
    def ewma(self) -> Optional[float]:
        return self._ewma
    
    def get_stats(self) -> BaselineStats:
        return BaselineStats(
            mean=self.mean,
            variance=self.variance,
            std=self.std,
            count=self.count,
            is_warm=self.is_warm,
            ewma=self._ewma
        )
    
    def reset(self) -> None:
        self._buffer.clear()
        self._mean = 0.0
        self._m2 = 0.0
        self._ewma = None
        self._total_count = 0
    
    def __repr__(self) -> str:
        warm_status = "warm" if self.is_warm else f"warming:{self.count}/{self.window_size}"
        return (
            f"BaselineComputer({warm_status}, "
            f"mean={self.mean:.4f}, std={self.std:.4f})"
        )


# --- FILE: src/blackice/cli.py ---

import argparse
import json
import sys
from pathlib import Path
from typing import Optional
import datetime
import yaml  # type: ignore

# RELATIVE IMPORTS for package execution
from blackice.pipeline import BlackicePipeline, PipelineConfig, stream_machine_data


def load_config(config_path: str) -> dict:
    with open(config_path, "r") as f:
        cfg = yaml.safe_load(f)
        # Ensure we have a dict even if yaml is empty
        return cfg if cfg else {}



def print_event(event):
    t = event.transition
    print(f"  [{t.timestamp}] {t.from_state.value} → {t.to_state.value} ({event.metric_name})")
    print(f"    Reason: {t.reason}")
    if t.zscore != 0:
        print(f"    Z-score: {t.zscore:.2f}")
    print()


def print_metrics(metrics: dict):
    print("\n" + "="*60)
    print("METRICS SUMMARY")
    print("="*60)
    
    print(f"\nMachine: {metrics['machine_id']}")
    print(f"Total Duration: {metrics['total_duration']} time units")
    
    sys_metrics = metrics["systems"]
    print("\n--- Systems Performance ---")
    print(f"  Rows Processed: {sys_metrics['rows_processed']:,}")
    print(f"  Total Time: {sys_metrics['total_time_seconds']:.2f}s")
    print(f"  Throughput: {sys_metrics['rows_per_second']:,.0f} rows/sec")
    print(f"  Peak Memory: {sys_metrics['peak_memory_mb']:.2f} MB")
    print(f"  Avg Time/Chunk: {sys_metrics['avg_time_per_chunk_ms']:.2f} ms")
    
    for metric_name in ["cpu", "memory"]:
        if metric_name in metrics:
            m = metrics[metric_name]
            print(f"\n--- {metric_name.upper()} Metrics ---")
            print(f"  Current State: {m['current_state']}")
            print(f"  Total Transitions: {m['transition_count']}")
            
            det = m["detection"]
            print("  Detection:")
            print(f"    Confirmed Shifts: {det['confirmed_shifts']}")
            print(f"    Rejected Spikes: {det['rejected_spikes']}")
            print(f"    Spike Rejection Rate: {det['spike_rejection_rate']:.1%}")
            print(f"    Avg Detection Latency: {det['detection_latency_mean']:.1f}")
            
            stab = m["stability"]
            print("  Stability:")
            print(f"    Total Regimes: {stab['total_regimes']}")
            print(f"    Avg Regime Duration: {stab['average_regime_duration']:.1f}")
            print(f"    Time in NORMAL: {stab['time_in_normal_pct']:.1f}%")
            print(f"    Time in UNSTABLE: {stab['time_in_unstable_pct']:.1f}%")
            print(f"    Time in SHIFTED: {stab['time_in_shifted_pct']:.1f}%")
    
    print("\n" + "="*60)

def generate_report(metrics: dict, config: dict, output_path: str):
    
    machine_id = metrics['machine_id']
    date_str = datetime.date.today().strftime('%Y-%m-%d')
    
    cpu = metrics.get('cpu', {})
    mem = metrics.get('memory', {})
    sys_m = metrics['systems']
    
    cpu_det = cpu.get('detection', {})
    mem_det = mem.get('detection', {})
    
    # Helper to get config values safely (handles flat vs nested)
    def get_cfg(section, key, default=None):
        if key in config: return config[key]
        return config.get(section, {}).get(key, default)

    min_consecutive = get_cfg('persistence', 'min_consecutive_points', 3)
    min_fraction = get_cfg('persistence', 'min_fraction_of_window', 0.5)
    z_threshold = get_cfg('deviation', 'zscore_threshold', 3.0)
    
    # cpu_stab = cpu.get('stability', {})  # Unused
    # mem_stab = mem.get('stability', {})  # Unused
    
    total_spikes = cpu_det.get('rejected_spikes', 0) + mem_det.get('rejected_spikes', 0)
    confirmed_shifts = cpu_det.get('confirmed_shifts', 0) + mem_det.get('confirmed_shifts', 0)
    
    is_healthy = confirmed_shifts == 0
    
    # health_status = "HEALTHY" if is_healthy else "UNHEALTHY"  # Unused
    health_icon = "✅" if is_healthy else "❌"
    
    if is_healthy:
        exec_summary = (
            f"**Machine {machine_id} is HEALTHY.** Despite detecting {total_spikes:,} instability events, "
            "the persistence filter correctly identified all of them as transient noise. "
            "No true regime shifts were confirmed.\n\n"
            "This analysis demonstrates the value of persistence-based filtering in production monitoring."
        )
        regime_section_title = "Why No Regime Changes Were Confirmed"
        regime_section_content = f"""### 1. Persistence Requirements Not Met
 
Configuration thresholds from `configs/default.yaml`:
 
| Check | Threshold | Observed |
|-------|-----------|----------|
| Consecutive points | ≥{min_consecutive} | ✗ Not met |
| Fraction of window | ≥{min_fraction:.0%} | ✗ Not met |
| Z-score threshold | >{z_threshold}σ | ✓ Met (triggered detection) |
 
While thousands of points exceeded the z-score threshold, **none persisted for {min_consecutive}+ consecutive points** — they returned to baseline before confirmation.
 
### 2. State Machine Behavior
 
```
NORMAL ──────┬──▶ UNSTABLE ──────┬──▶ NORMAL (spike rejected)
             │                   │
        Deviation           Return to
        detected            baseline
        (|z| > {z_threshold}σ)          (< {min_consecutive} points)
```

Every transition followed the pattern:
1. `NORMAL → UNSTABLE`: Significant deviation detected
2. `UNSTABLE → NORMAL`: Deviation did not persist, noise filtered

### 3. Volatility Evidence

The high number of UNSTABLE entries ({total_spikes:,} total) indicates:
- **High operational volatility** — frequent but brief spikes
- **Stable baseline** — system always returns to normal operating range
- **No drift** — the underlying baseline is not shifting"""
        
        infra_diagnosis = (
            "**Most likely**: The machine runs a **bursty workload** (e.g., request-driven service, "
            "batch jobs with quick completion) that causes frequent but transient metric spikes."
        )
        
        sre_assessment = f"""**Assessment: No.**

| Factor | Evaluation |
|--------|------------|
| Severity | Low — No confirmed regime shift |
| Urgency | None — All deviations self-resolved |
| Impact | Minimal — Normal operational variance |
| Action | None required |

**Recommendation**: 
- **No alert** — Machine is operating normally
- **Monitor trend** — If spike frequency increases significantly, investigate
- **Consider tuning** — The high detection count suggests the z-score threshold ({z_threshold}σ) may be too sensitive for this workload.
"""
        risk_table = """| Risk | Level | Notes |
|------|-------|-------|
| OOM kill | ✅ Low | No memory pressure trend |
| Performance degradation | ✅ Low | Spikes resolve quickly |
| Cascading failure | ✅ Low | No sustained abnormal state |
| Capacity concern | ✅ Low | Operating within normal bounds |"""

    else:
        exec_summary = (
            f"**Machine {machine_id} is UNHEALTHY.** The system detected {confirmed_shifts} confirmed regime shifts. "
            "This indicates structural changes in the workload or underlying resource usage."
        )
        regime_section_title = "Detected Regime Shifts"
        regime_section_content = "Confirmations detected. Please inspect event logs for details."
        infra_diagnosis = "Machine shows signs of structural instability."
        sre_assessment = "**Assessment: YES. Investigation required.**"
        risk_table = "High risk detected."

    report_content = f"""# Incident Analysis — Machine {machine_id}

**Analysis Date**: {date_str}  
**System**: BLACKICE Regime Detection v1.0  
**Configuration**: `Learned Parameters (Hybrid ML)`


---

## Executive Summary

{exec_summary}

---

## Signal Summary

### CPU Behavior
- **Pattern**: High-frequency oscillation with frequent threshold crossings
- **Baseline range**: Variable, with {cpu_det.get('rejected_spikes', 0):,} transient deviations detected
- **Regime status**: {health_icon} {cpu.get('current_state', 'UNKNOWN')} — {'all deviations filtered as noise' if is_healthy else 'shifts detected'}
- **Volatility**: High but consistent (no structural change)

### Memory Behavior
- **Pattern**: Similar oscillatory pattern to CPU
- **Baseline range**: Variable, with {mem_det.get('rejected_spikes', 0):,} transient deviations detected
- **Regime status**: {health_icon} {mem.get('current_state', 'UNKNOWN')} — {'all deviations filtered as noise' if is_healthy else 'shifts detected'}
- **Volatility**: High but consistent

---

## Detection Statistics

| Metric | CPU | Memory | Total |
|--------|-----|--------|-------|
| **Data Points** | {sys_m.get('rows_processed', 0):,} | {sys_m.get('rows_processed', 0):,} | {sys_m.get('rows_processed', 0):,} |
| **Time Span** | {metrics.get('total_duration', 0):,} | {metrics.get('total_duration', 0):,} | {metrics.get('total_duration', 0):,} |
| **Instability Events** | {cpu_det.get('rejected_spikes', 0):,} | {mem_det.get('rejected_spikes', 0):,} | {total_spikes:,} |
| **Confirmed Shifts** | {cpu_det.get('confirmed_shifts', 0)} | {mem_det.get('confirmed_shifts', 0)} | {confirmed_shifts} |
| **Rejection Rate** | {cpu_det.get('spike_rejection_rate', 0):.0%} | {mem_det.get('spike_rejection_rate', 0):.0%} | 100% |
| **Final State** | {cpu.get('current_state', 'UNKNOWN')} | {mem.get('current_state', 'UNKNOWN')} | — |

---

## {regime_section_title}

{regime_section_content}

---

## Rejected Noise Summary

### Spike Filtering Statistics

| Metric | CPU | Memory |
|--------|-----|--------|
| Transient spikes detected | {cpu_det.get('rejected_spikes', 0):,} | {mem_det.get('rejected_spikes', 0):,} |
| Spikes rejected | {cpu_det.get('rejected_spikes', 0):,} (100%) | {mem_det.get('rejected_spikes', 0):,} (100%) |
| Confirmed as regime shifts | {cpu_det.get('confirmed_shifts', 0)} | {mem_det.get('confirmed_shifts', 0)} |

### Pattern Analysis

The 100% rejection rate indicates:
- All instabilities were **transient** (duration < {min_consecutive} points)
- The system exhibits **bursty behavior** but maintains baseline
- **No memory leaks, saturation, or workload shifts** detected

### Why 100% Rejection Is Not a Failure

A 100% rejection rate is expected for machines with bursty but stable workloads.
The goal of BLACKICE is not to minimize rejections, but to minimize false positives
that would cause unnecessary operational action.

In this case:
- Deviations were frequent but short-lived
- No sustained variance or mean shift was observed
- Persistence thresholds correctly prevented alert fatigue

**This behavior is desirable in production monitoring systems.**

---

## Scalability & Performance

| Metric | Value |
|--------|-------|
| Rows processed | {sys_m.get('rows_processed', 0):,} |
| Processing time | {sys_m.get('total_time_seconds', 0):.2f} seconds |
| Throughput | **{sys_m.get('rows_per_second', 0):,.0f} rows/second** |
| Memory model | Constant O(window_size) |

This throughput was achieved using constant memory and chunked processing.
The pipeline scales linearly with input size and is suitable for multi-million
row datasets without architectural changes.

---

## Infra Interpretation

### Diagnosis

Machine {machine_id} exhibits characteristics of a **high-variance but stable workload**:

| Hypothesis | Evidence | Verdict |
|------------|----------|---------|
| Memory leak | No upward drift, returns to baseline | ❌ Ruled out |
| Workload shift | No persistent mean change | ❌ Ruled out |
| Resource saturation | No sustained high utilization | ❌ Ruled out |
| Bursty workload | Frequent spikes, quick recovery | ✅ Likely |

{infra_diagnosis}

### Would This Trigger an SRE Page?

{sre_assessment}

### Operational Risk Assessment

{risk_table}

---

## Performance Metrics

| Metric | Value |
|--------|-------|
| Rows processed | {sys_m.get('rows_processed', 0):,} |
| Processing time | {sys_m.get('total_time_seconds', 0):.2f} seconds |
| Throughput | {sys_m.get('rows_per_second', 0):,.0f} rows/second |
| Chunks processed | (streaming) |

---

## Appendix: Detection Configuration

```yaml
# configs/default.yaml
baseline:
  window_size: {get_cfg('baseline', 'window_size', 60)}
  use_ewma: {str(get_cfg('baseline', 'use_ewma', False)).lower()}

deviation:
  zscore_threshold: {z_threshold}

persistence:
  min_consecutive_points: {min_consecutive}
  min_fraction_of_window: {min_fraction}
```

---

## Appendix: Key Takeaways

1. **Persistence filtering is critical** — Without it, {machine_id} would have generated {total_spikes:,} false alarms
2. **High spike count ≠ unhealthy** — Volatility without persistence indicates bursty but stable behavior
3. **100% rejection rate is valid** — It means the system correctly identified all deviations as transient noise

---

*Report generated by BLACKICE Regime Detection System*
"""
    
    with open(output_path, "w") as f:
        f.write(report_content)
    print(f"\\nReport saved to: {output_path}")

def run_pipeline(
    data_path: str,
    machine_id: str,
    config: dict,
    verbose: bool = False,
    output_path: Optional[str] = None,
    report_file: Optional[str] = None
):
    
    print("BLACKICE Regime Detection System")
    print(f"Machine: {machine_id}")
    print(f"Data: {data_path}")
    print("-" * 40)
    
    pipeline_config = PipelineConfig.from_dict(config)
    pipeline = BlackicePipeline(pipeline_config)
    
    chunksize = config.get("data", {}).get("chunksize", 500000)
    
    chunk_count = 0
    total_events = 0
    
    print("\\nProcessing data in streaming mode...")
    
    for chunk in stream_machine_data(data_path, machine_id, chunksize=chunksize):
        chunk_count += 1
        events = pipeline.process_chunk(chunk)
        
        if events:
            total_events += len(events)
            if verbose:
                print(f"\\nChunk {chunk_count}: {len(events)} events detected")
                for event in events:
                    print_event(event)
        
        if chunk_count % 10 == 0:
            print(f"  Processed chunk {chunk_count}...")
    
    pipeline.stop()
    
    print("\\nProcessing complete!")
    print(f"  Chunks: {chunk_count}")
    print(f"  Events: {total_events}")
    
    metrics = pipeline.get_all_metrics()
    print_metrics(metrics)
    
    if output_path:
        output = {
            "machine_id": machine_id,
            "config": config,
            "metrics": metrics,
            "events": [e.to_dict() for e in pipeline.events]
        }
        with open(output_path, "w") as f:
            json.dump(output, f, indent=2)
        print(f"\\nResults saved to: {output_path}")
        
    if report_file:
        generate_report(metrics, config, report_file)
    
    return pipeline


def main():
    parser = argparse.ArgumentParser(
        description="BLACKICE - Infrastructure Regime Detection System"
    )
    parser.add_argument(
        "--config", "-c",
        default="configs/default.yaml",
        help="Path to configuration YAML file"
    )
    parser.add_argument(
        "--machine", "-m",
        help="Machine ID to analyze (overrides config)"
    )
    parser.add_argument(
        "--data", "-d",
        help="Path to machine_usage.csv (overrides config)"
    )
    parser.add_argument(
        "--output", "-o",
        help="Path to save JSON results"
    )
    parser.add_argument(
        "--report", "-r",
        action="store_true",
        help="Generate markdown report (default: reports/analysis_<machine_id>.md)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Print detailed event information"
    )
    
    args = parser.parse_args()
    
    config_path = Path(args.config)
    
    # Try current directory first, but if running installed, we might want defaults
    if not config_path.exists():
         # Fallback logic could go here, for now relying on local execution or explicit paths
         pass
    
    if not config_path.exists():
        print(f"Error: Config file not found: {args.config}")
        print("Please provide a path to a valid config file.")
        sys.exit(1)
    
    config = load_config(str(config_path))
    
    data_path = args.data or config.get("data", {}).get("machine_usage_path", "machine_usage.csv")
    machine_id = args.machine or config.get("data", {}).get("target_machine_id", "m_1932")
    
    if not Path(data_path).exists():
        print(f"Error: Data file not found: {data_path}")
        print("Please provide a path to a valid csv data file.")
        sys.exit(1)
    
    report_file = None
    if args.report:
        # Save report to current directory reports/ by default if not specified
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)
        report_file = str(reports_dir / f"analysis_{machine_id}.md")
    
    run_pipeline(
        data_path=data_path,
        machine_id=machine_id,
        config=config,
        verbose=args.verbose,
        output_path=args.output,
        report_file=report_file
    )


if __name__ == "__main__":
    main()


# --- FILE: src/blackice/detector.py ---

import time
from dataclasses import dataclass
from typing import Optional

from .baseline import BaselineComputer
from .deviation import DeviationTracker
from .persistence import PersistenceValidator, PersistenceConfig
from .state import RegimeStateMachine, RegimeState

@dataclass
class DetectionEvent:
    """
    A high-level event representing the system's current regime status.
    Returned by RegimeDetector.update().
    """
    timestamp: float
    value: float
    zscore: float
    state: RegimeState
    reason: str
    duration: float  # Duration in current state in seconds
    is_anomaly: bool # Helper property: True if not NORMAL

class RegimeDetector:
    """
    The main entry point for BLACKICE.
    
    Wraps the statistical engine (Baseline, Deviation, Persistence, State Machine)
    into a single, easy-to-use object.
    
    Usage:
        detector = RegimeDetector(window_size=60, z_threshold=3.0)
        event = detector.update(cpu_usage)
        if event.is_anomaly:
            print(f"Anomaly detected: {event.reason}")
    """
    
    def __init__(
        self, 
        window_size: int = 60, 
        z_threshold: float = 3.0,
        persistence: int = 10,
        min_fraction: float = 0.1,
        metric_name: str = "metric"
    ):
        """
        Initialize the detector with configuration.
        
        Args:
            window_size: Number of samples for the sliding window baseline.
            z_threshold: Sigma threshold for outlier detection (e.g., 3.0).
            persistence: Minimum consecutive outliers to confirm a regime shift.
            min_fraction: Minimum fraction of outliers in window (e.g., 0.1).
            metric_name: Label for the metric (used in logs/reasons).
        """
        self.metric_name = metric_name
        
        # 1. Baseline Computer (O(1) Welford's Algorithm)
        self.baseline = BaselineComputer(window_size=window_size)
        
        # 2. Deviation Tracker (Z-Score monitoring)
        self.deviation = DeviationTracker(
            baseline=self.baseline, 
            zscore_threshold=z_threshold
        )
        
        # 3. Persistence Validator (Noise filtering)
        self.persistence = PersistenceValidator(
            PersistenceConfig(
                min_consecutive_points=persistence,
                min_fraction_of_window=min_fraction,
                window_size=window_size
            )
        )
        
        # 4. State Machine (Deterministic regimes)
        self.sm = RegimeStateMachine(metric_name=metric_name)
        
        # Internal tracking for duration
        self._state_start_ts: Optional[float] = None
        self._last_state: RegimeState = RegimeState.NORMAL
        
    def update(self, value: float, timestamp: Optional[float] = None) -> DetectionEvent:
        """
        Process a new data point and return the current system state.
        
        Args:
            value: The numeric metric value (e.g., CPU %, latency ms).
            timestamp: Optional epoch timestamp. Defaults to time.time().
            
        Returns:
            DetectionEvent object containing state, z-score, duration, etc.
        """
        if timestamp is None:
            timestamp = time.time()
            
        # 0. Initial state timestamp tracking
        if self._state_start_ts is None:
            self._state_start_ts = timestamp
            
        # 1. Update internals (Chain of Responsibility)
        dev_result = self.deviation.update(value, int(timestamp))
        persist_result = self.persistence.check(dev_result)
        
        # 2. Process State Machine
        # Note: process() returns a transaction IF state changed, but we want current state
        transition = self.sm.process(persist_result, int(timestamp), zscore=dev_result.zscore)
        current_state = self.sm.current_state
        
        # 3. Handle Duration Logic
        if current_state != self._last_state:
            self._state_start_ts = timestamp
            self._last_state = current_state
            
        duration = timestamp - self._state_start_ts
        
        # 4. Construct high-level event
        reason = "Normal operation"
        if current_state == RegimeState.UNSTABLE:
            reason = f"Volatility detected (z={dev_result.zscore:.1f})"
        elif current_state == RegimeState.SHIFTED:
            reason = f"regime shift confirmed (duration={duration:.1f}s)"
            
        # Override reason if we just transitioned
        if transition:
            reason = transition.reason
            
        return DetectionEvent(
            timestamp=timestamp,
            value=value,
            zscore=dev_result.zscore,
            state=current_state,
            reason=reason,
            duration=duration,
            is_anomaly=(current_state != RegimeState.NORMAL)
        )
        
    @property
    def is_calibrated(self) -> bool:
        """True if the baseline window is full and statistics are reliable."""
        return self.baseline.is_ready


# --- FILE: src/blackice/deviation.py ---

from dataclasses import dataclass
from enum import Enum
from typing import Optional

from .baseline import BaselineComputer


class DeviationDirection(Enum):
    NONE = "NONE"
    HIGH = "HIGH"
    LOW = "LOW"


@dataclass
class DeviationResult:
    timestamp: int
    value: float
    zscore: float
    magnitude: float
    direction: DeviationDirection
    consecutive_count: int
    deviation_start_ts: Optional[int]
    is_significant: bool
    
    @property
    def duration(self) -> int:
        if self.deviation_start_ts is None:
            return 0
        return self.timestamp - self.deviation_start_ts


class DeviationTracker:
    
    def __init__(
        self, 
        baseline: BaselineComputer,
        zscore_threshold: float = 2.0
    ):
        self.baseline = baseline
        self.zscore_threshold = zscore_threshold
        
        self._consecutive_deviations: int = 0
        self._deviation_start_ts: Optional[int] = None
        self._current_direction: DeviationDirection = DeviationDirection.NONE
        self._last_significant_zscore: float = 0.0
    
    def compute_zscore(self, value: float) -> float:
        std = self.baseline.std
        if std < 1e-6:
            return 0.0
        return (value - self.baseline.mean) / std
    
    def update(self, value: float, timestamp: int) -> DeviationResult:
        zscore = self.compute_zscore(value)
        magnitude = abs(zscore)
        
        if magnitude >= self.zscore_threshold:
            direction = DeviationDirection.HIGH if zscore > 0 else DeviationDirection.LOW
            is_significant = True
            
            if self._current_direction == direction:
                self._consecutive_deviations += 1
            else:
                self._consecutive_deviations = 1
                self._deviation_start_ts = timestamp
                self._current_direction = direction
            
            self._last_significant_zscore = zscore
        else:
            direction = DeviationDirection.NONE
            is_significant = False
            self._consecutive_deviations = 0
            self._deviation_start_ts = None
            self._current_direction = DeviationDirection.NONE
        
        self.baseline.update(value)
        
        return DeviationResult(
            timestamp=timestamp,
            value=value,
            zscore=zscore,
            magnitude=magnitude,
            direction=direction,
            consecutive_count=self._consecutive_deviations,
            deviation_start_ts=self._deviation_start_ts,
            is_significant=is_significant
        )
    
    @property
    def consecutive_deviations(self) -> int:
        return self._consecutive_deviations
    
    @property
    def current_direction(self) -> DeviationDirection:
        return self._current_direction
    
    @property
    def last_significant_zscore(self) -> float:
        return self._last_significant_zscore
    
    def reset(self) -> None:
        self._consecutive_deviations = 0
        self._deviation_start_ts = None
        self._current_direction = DeviationDirection.NONE
        self._last_significant_zscore = 0.0
    
    def __repr__(self) -> str:
        return (
            f"DeviationTracker(threshold={self.zscore_threshold}, "
            f"consecutive={self._consecutive_deviations}, "
            f"direction={self._current_direction.value})"
        )


# --- FILE: src/blackice/learning/__init__.py ---


# --- FILE: src/blackice/learning/objective.py ---
from typing import List
from dataclasses import dataclass
from blackice.state import StateEvent

@dataclass
class AnomalyInterval:
    """Represents a time range where a true anomaly exists."""
    start_time: float
    end_time: float
    
    def contains(self, timestamp: float) -> bool:
        return self.start_time <= timestamp <= self.end_time

def calculate_loss(
    events: List[StateEvent], 
    ground_truth: List[AnomalyInterval],
    penalty_fp: float = 5.0,
    penalty_fn: float = 10.0,
    penalty_delay: float = 0.1
) -> float:
    """
    Calculates the loss (cost) of a detection run against ground truth.
    
    Loss = (False Positives * W_FP) + (False Negatives * W_FN) + (Avg Delay * W_Delay)
    
    Args:
        events: List of detection events produced by the pipeline.
        ground_truth: List of time intervals that are TRUE anomalies.
        penalty_fp: Cost of a False Positive (Alert where no anomaly exists).
        penalty_fn: Cost of a False Negative (Missed anomaly interval).
        penalty_delay: Cost per second of delay in detecting the start of an anomaly.
        
    Returns:
        float: Total loss scores (lower is better).
    """
    false_positives = 0
    detected_intervals = set()
    total_delay = 0.0
    
    # 1. Evaluate Precision (False Positives & Delay)
    for event in events:
        timestamp = event.transition.timestamp
        matched = False
        
        for i, interval in enumerate(ground_truth):
            if interval.contains(timestamp):
                matched = True
                detected_intervals.add(i)
                # Calculate delay only for the first detection in an interval
                # (Simple heuristic: assumes events are sorted)
                delay = max(0, timestamp - interval.start_time)
                total_delay += delay
                break
        
        if not matched:
            false_positives += 1
            
    # 2. Evaluate Recall (False Negatives)
    false_negatives = len(ground_truth) - len(detected_intervals)
    
    # 3. Calculate Weighted Loss
    loss = (
        (false_positives * penalty_fp) +
        (false_negatives * penalty_fn) +
        (total_delay * penalty_delay)
    )
    
    return loss


# --- FILE: src/blackice/learning/optimizer.py ---
from typing import List, Dict, Any, Optional
import itertools
import pandas as pd
from dataclasses import asdict
import logging

from blackice.pipeline import BlackicePipeline, PipelineConfig
from blackice.learning.objective import calculate_loss, AnomalyInterval

logger = logging.getLogger(__name__)

class GridSearchOptimizer:
    """
    Performs a Grid Search to find the optimal BlackIce configuration
    that minimizes the loss function against a Ground Truth dataset.
    """
    
    def __init__(
        self, 
        param_grid: Optional[Dict[str, List[Any]]] = None
    ):
        self.param_grid = param_grid or {
            "window_size": [10, 20, 50, 100],
            "zscore_threshold": [2.0, 3.0, 4.0, 5.0],
            "min_consecutive_points": [3, 5, 8, 12]
        }
        
    def _generate_configs(self) -> List[PipelineConfig]:
        """Generates all combinations of parameters."""
        keys = self.param_grid.keys()
        values = self.param_grid.values()
        combinations = itertools.product(*values)
        
        configs = []
        for combo in combinations:
            params = dict(zip(keys, combo))
            # Create config object (handling defaults for others)
            config = PipelineConfig(
                window_size=params["window_size"],
                zscore_threshold=params["zscore_threshold"],
                min_consecutive_points=params["min_consecutive_points"]
            )
            configs.append(config)
        return configs

    def train(
        self, 
        df: pd.DataFrame, 
        ground_truth: List[AnomalyInterval]
    ) -> Optional[PipelineConfig]:
        """
        Runs the optimization loop.
        
        Args:
            df: Historical training data.
            ground_truth: Known anomaly intervals.
            
        Returns:
            PipelineConfig: The best configuration found.
        """
        configs = self._generate_configs()
        best_loss = float('inf')
        best_config = None
        
        print(f"Starting Grid Search over {len(configs)} configurations...")
        
        for i, config in enumerate(configs):
            # 1. Initialize Pipeline
            pipeline = BlackicePipeline(config)
            
            # 2. Run Inference (Offline Training Mode)
            # We treat the whole df as one stream source per machine
            events = pipeline.process_chunk(df)
            
            # 3. Calculate Loss
            loss = calculate_loss(events, ground_truth)
            
            print(f"[{i+1}/{len(configs)}] Params: {asdict(config)} -> Loss: {loss:.4f}")
            
            # 4. Update Best
            if loss < best_loss:
                best_loss = loss
                best_config = config
                
        print("\nOptimization Complete.")
        if best_config:
            print(f"Best Loss: {best_loss:.4f}")
            print(f"Best Config: {asdict(best_config)}")
        else:
            print("Optimization failed: No valid config found.")
        
        return best_config


# --- FILE: src/blackice/metrics.py ---

from dataclasses import dataclass
from typing import List, Dict, Optional
import time
import tracemalloc

from .state import StateTransition, RegimeState


@dataclass
class DetectionQualityMetrics:
    detection_latency_mean: float = 0.0
    detection_latency_max: float = 0.0
    spike_rejection_rate: float = 0.0
    confirmed_shifts: int = 0
    rejected_spikes: int = 0
    
    def to_dict(self) -> dict:
        return {
            "detection_latency_mean": self.detection_latency_mean,
            "detection_latency_max": self.detection_latency_max,
            "spike_rejection_rate": self.spike_rejection_rate,
            "confirmed_shifts": self.confirmed_shifts,
            "rejected_spikes": self.rejected_spikes
        }


@dataclass
class StabilityMetrics:
    mean_time_between_regimes: float = 0.0
    average_regime_duration: float = 0.0
    total_regimes: int = 0
    time_in_normal_pct: float = 0.0
    time_in_unstable_pct: float = 0.0
    time_in_shifted_pct: float = 0.0
    
    def to_dict(self) -> dict:
        return {
            "mean_time_between_regimes": self.mean_time_between_regimes,
            "average_regime_duration": self.average_regime_duration,
            "total_regimes": self.total_regimes,
            "time_in_normal_pct": self.time_in_normal_pct,
            "time_in_unstable_pct": self.time_in_unstable_pct,
            "time_in_shifted_pct": self.time_in_shifted_pct
        }


@dataclass  
class SystemsMetrics:
    rows_processed: int = 0
    total_time_seconds: float = 0.0
    rows_per_second: float = 0.0
    peak_memory_mb: float = 0.0
    avg_time_per_chunk_ms: float = 0.0
    chunks_processed: int = 0
    
    def to_dict(self) -> dict:
        return {
            "rows_processed": self.rows_processed,
            "total_time_seconds": self.total_time_seconds,
            "rows_per_second": self.rows_per_second,
            "peak_memory_mb": self.peak_memory_mb,
            "avg_time_per_chunk_ms": self.avg_time_per_chunk_ms,
            "chunks_processed": self.chunks_processed
        }


class MetricsComputer:
    
    def __init__(self, track_memory: bool = True):
        self.track_memory = track_memory
        
        self._start_time: Optional[float] = None
        self._chunk_times: List[float] = []
        self._rows_processed: int = 0
        self._peak_memory: float = 0.0
        
        self._unstable_entries: int = 0
        self._shifted_entries: int = 0
        self._normal_returns: int = 0
        
        if track_memory:
            tracemalloc.start()
    
    def start_processing(self) -> None:
        self._start_time = time.time()
    
    def record_chunk(self, rows: int, duration: float) -> None:
        self._rows_processed += rows
        self._chunk_times.append(duration)
        
        if self.track_memory:
            current, peak = tracemalloc.get_traced_memory()
            self._peak_memory = max(self._peak_memory, peak / 1024 / 1024)
    
    def record_transition(self, transition: StateTransition) -> None:
        if transition.to_state == RegimeState.UNSTABLE:
            self._unstable_entries += 1
        elif transition.to_state == RegimeState.SHIFTED:
            self._shifted_entries += 1
        elif transition.to_state == RegimeState.NORMAL:
            self._normal_returns += 1
    
    def compute_detection_quality(
        self, 
        transitions: List[StateTransition]
    ) -> DetectionQualityMetrics:
        latencies = []
        confirmed = 0
        rejected = 0
        
        unstable_start = None
        
        for t in transitions:
            if t.to_state == RegimeState.UNSTABLE:
                unstable_start = t.timestamp
            elif t.to_state == RegimeState.SHIFTED and unstable_start is not None:
                latencies.append(t.timestamp - unstable_start)
                confirmed += 1
                unstable_start = None
            elif t.to_state == RegimeState.NORMAL and t.from_state == RegimeState.UNSTABLE:
                rejected += 1
                unstable_start = None
        
        for t in transitions:
            if t.to_state == RegimeState.SHIFTED and t.from_state == RegimeState.NORMAL:
                confirmed += 1
        
        total_unstable = confirmed + rejected
        rejection_rate = rejected / total_unstable if total_unstable > 0 else 0.0
        
        return DetectionQualityMetrics(
            detection_latency_mean=sum(latencies) / len(latencies) if latencies else 0.0,
            detection_latency_max=max(latencies) if latencies else 0.0,
            spike_rejection_rate=rejection_rate,
            confirmed_shifts=confirmed,
            rejected_spikes=rejected
        )
    
    def compute_stability(
        self, 
        transitions: List[StateTransition],
        total_duration: int
    ) -> StabilityMetrics:
        if not transitions:
            return StabilityMetrics(
                time_in_normal_pct=100.0
            )
        
        shift_times = [
            t.timestamp for t in transitions 
            if t.to_state == RegimeState.SHIFTED
        ]
        
        regime_intervals = []
        for i in range(1, len(shift_times)):
            regime_intervals.append(shift_times[i] - shift_times[i-1])
        
        regime_durations = []
        shifted_start = None
        for t in transitions:
            if t.to_state == RegimeState.SHIFTED:
                shifted_start = t.timestamp
            elif shifted_start is not None and t.from_state == RegimeState.SHIFTED:
                regime_durations.append(t.timestamp - shifted_start)
                shifted_start = None
        
        time_in_normal = 0
        time_in_unstable = 0
        time_in_shifted = 0
        
        sorted_trans = sorted(transitions, key=lambda x: x.timestamp)
        if sorted_trans:
            prev_time = 0
            prev_state = RegimeState.NORMAL
            
            for t in sorted_trans:
                duration = t.timestamp - prev_time
                if prev_state == RegimeState.NORMAL:
                    time_in_normal += duration
                elif prev_state == RegimeState.UNSTABLE:
                    time_in_unstable += duration
                elif prev_state == RegimeState.SHIFTED:
                    time_in_shifted += duration
                prev_time = t.timestamp
                prev_state = t.to_state
            
            final_duration = total_duration - prev_time
            if prev_state == RegimeState.NORMAL:
                time_in_normal += final_duration
            elif prev_state == RegimeState.UNSTABLE:
                time_in_unstable += final_duration
            elif prev_state == RegimeState.SHIFTED:
                time_in_shifted += final_duration
        
        total_time = time_in_normal + time_in_unstable + time_in_shifted
        if total_time == 0:
            total_time = 1
        
        return StabilityMetrics(
            mean_time_between_regimes=sum(regime_intervals) / len(regime_intervals) if regime_intervals else 0.0,
            average_regime_duration=sum(regime_durations) / len(regime_durations) if regime_durations else 0.0,
            total_regimes=len(shift_times),
            time_in_normal_pct=(time_in_normal / total_time) * 100,
            time_in_unstable_pct=(time_in_unstable / total_time) * 100,
            time_in_shifted_pct=(time_in_shifted / total_time) * 100
        )
    
    def compute_systems_metrics(self) -> SystemsMetrics:
        total_time = time.time() - self._start_time if self._start_time else 0.0
        
        return SystemsMetrics(
            rows_processed=self._rows_processed,
            total_time_seconds=total_time,
            rows_per_second=self._rows_processed / total_time if total_time > 0 else 0.0,
            peak_memory_mb=self._peak_memory,
            avg_time_per_chunk_ms=(sum(self._chunk_times) / len(self._chunk_times) * 1000) if self._chunk_times else 0.0,
            chunks_processed=len(self._chunk_times)
        )
    
    def stop_tracking(self) -> None:
        if self.track_memory:
            tracemalloc.stop()
    
    def reset(self) -> None:
        self._start_time = None
        self._chunk_times.clear()
        self._rows_processed = 0
        self._peak_memory = 0.0
        self._unstable_entries = 0
        self._shifted_entries = 0
        self._normal_returns = 0


def compute_variance_shift(
    pre_values: List[float], 
    post_values: List[float]
) -> Dict[str, float]:
    import statistics
    
    if len(pre_values) < 2 or len(post_values) < 2:
        return {"pre_variance": 0.0, "post_variance": 0.0, "ratio": 1.0}
    
    pre_var = statistics.variance(pre_values)
    post_var = statistics.variance(post_values)
    
    ratio = post_var / pre_var if pre_var > 0 else float('inf')
    
    return {
        "pre_variance": pre_var,
        "post_variance": post_var,
        "ratio": ratio,
        "significant_change": abs(ratio - 1.0) > 0.5
    }


# --- FILE: src/blackice/persistence.py ---

from dataclasses import dataclass
from enum import Enum
from typing import Optional

from .deviation import DeviationResult, DeviationDirection


class PersistenceStatus(Enum):
    NOT_DEVIATING = "NOT_DEVIATING"
    WATCHING = "WATCHING"
    CONFIRMED = "CONFIRMED"


@dataclass
class PersistenceConfig:
    min_consecutive_points: int = 10
    min_fraction_of_window: float = 0.3
    window_size: int = 60
    
    @property
    def effective_threshold(self) -> int:
        fractional = int(self.min_fraction_of_window * self.window_size)
        return max(self.min_consecutive_points, fractional)


@dataclass
class PersistenceResult:
    status: PersistenceStatus
    consecutive_count: int
    required_count: int
    direction: DeviationDirection
    deviation_start_ts: Optional[int]
    confirmation_ts: Optional[int]
    progress_fraction: float
    
    @property
    def is_confirmed(self) -> bool:
        return self.status == PersistenceStatus.CONFIRMED


class PersistenceValidator:
    
    def __init__(self, config: PersistenceConfig):
        self.config = config
        
        self._watching: bool = False
        self._watch_start_ts: Optional[int] = None
        self._confirmed: bool = False
        self._confirmation_ts: Optional[int] = None
        self._last_direction: DeviationDirection = DeviationDirection.NONE
    
    def check(self, deviation: DeviationResult) -> PersistenceResult:
        required = self.config.effective_threshold
        
        if not deviation.is_significant:
            status = PersistenceStatus.NOT_DEVIATING
            self._watching = False
            self._confirmed = False
            self._watch_start_ts = None
            self._confirmation_ts = None
            self._last_direction = DeviationDirection.NONE
            
            return PersistenceResult(
                status=status,
                consecutive_count=0,
                required_count=required,
                direction=DeviationDirection.NONE,
                deviation_start_ts=None,
                confirmation_ts=None,
                progress_fraction=0.0
            )
        
        if deviation.direction != self._last_direction:
            self._watching = True
            self._watch_start_ts = deviation.deviation_start_ts
            self._confirmed = False
            self._confirmation_ts = None
            self._last_direction = deviation.direction
        
        consecutive = deviation.consecutive_count
        progress = consecutive / required if required > 0 else 0.0
        
        if consecutive >= required:
            status = PersistenceStatus.CONFIRMED
            if not self._confirmed:
                self._confirmed = True
                self._confirmation_ts = deviation.timestamp
        else:
            status = PersistenceStatus.WATCHING
        
        return PersistenceResult(
            status=status,
            consecutive_count=consecutive,
            required_count=required,
            direction=deviation.direction,
            deviation_start_ts=self._watch_start_ts,
            confirmation_ts=self._confirmation_ts,
            progress_fraction=min(1.0, progress)
        )
    
    @property
    def is_watching(self) -> bool:
        return self._watching
    
    @property
    def is_confirmed(self) -> bool:
        return self._confirmed
    
    def reset(self) -> None:
        self._watching = False
        self._watch_start_ts = None
        self._confirmed = False
        self._confirmation_ts = None
        self._last_direction = DeviationDirection.NONE
    
    def __repr__(self) -> str:
        status = "CONFIRMED" if self._confirmed else ("WATCHING" if self._watching else "IDLE")
        return f"PersistenceValidator(status={status}, threshold={self.config.effective_threshold})"


# --- FILE: src/blackice/pipeline.py ---

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Iterator, Any
import time
import pandas as pd

from .baseline import BaselineComputer
from .deviation import DeviationTracker
from .persistence import PersistenceValidator, PersistenceConfig
from .state import RegimeStateMachine, StateTransition, StateEvent, RegimeState
from .metrics import MetricsComputer


@dataclass
class PipelineConfig:
    window_size: int = 60
    use_ewma: bool = False
    ewma_alpha: float = 0.3
    zscore_threshold: float = 2.0
    min_consecutive_points: int = 10
    min_fraction_of_window: float = 0.3
    track_cpu: bool = True
    track_memory: bool = True
    
    @classmethod
    def from_dict(cls, config: dict) -> "PipelineConfig":
        baseline = config.get("baseline", {})
        deviation = config.get("deviation", {})
        persistence = config.get("persistence", {})
        metrics = config.get("metrics", {})
        
        return cls(
            window_size=baseline.get("window_size", 60),
            use_ewma=baseline.get("use_ewma", False),
            ewma_alpha=baseline.get("ewma_alpha", 0.3),
            zscore_threshold=deviation.get("zscore_threshold", 2.0),
            min_consecutive_points=persistence.get("min_consecutive_points", 10),
            min_fraction_of_window=persistence.get("min_fraction_of_window", 0.3),
            track_cpu=metrics.get("cpu", True),
            track_memory=metrics.get("memory", True)
        )


@dataclass
class MetricTracker:
    name: str
    baseline: BaselineComputer
    deviation: DeviationTracker
    persistence: PersistenceValidator
    state_machine: RegimeStateMachine
    
    values: List[float] = field(default_factory=list)
    timestamps: List[int] = field(default_factory=list)
    zscores: List[float] = field(default_factory=list)
    means: List[float] = field(default_factory=list)
    stds: List[float] = field(default_factory=list)


class BlackicePipeline:
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        
        self._trackers: Dict[str, MetricTracker] = {}
        
        if config.track_cpu:
            self._trackers["cpu"] = self._create_tracker("cpu")
        if config.track_memory:
            self._trackers["memory"] = self._create_tracker("memory")
        
        self._metrics = MetricsComputer(track_memory=True)
        self._events: List[StateEvent] = []
        self._machine_id: str = ""
        self._first_timestamp: Optional[int] = None
        self._last_timestamp: Optional[int] = None
        self._started: bool = False
    
    def _create_tracker(self, name: str) -> MetricTracker:
        baseline = BaselineComputer(
            window_size=self.config.window_size,
            use_ewma=self.config.use_ewma,
            ewma_alpha=self.config.ewma_alpha
        )
        
        deviation = DeviationTracker(
            baseline=baseline,
            zscore_threshold=self.config.zscore_threshold
        )
        
        persistence_config = PersistenceConfig(
            min_consecutive_points=self.config.min_consecutive_points,
            min_fraction_of_window=self.config.min_fraction_of_window,
            window_size=self.config.window_size
        )
        persistence = PersistenceValidator(persistence_config)
        
        state_machine = RegimeStateMachine(metric_name=name)
        
        return MetricTracker(
            name=name,
            baseline=baseline,
            deviation=deviation,
            persistence=persistence,
            state_machine=state_machine
        )
    
    def process_chunk(self, df_chunk: pd.DataFrame) -> List[StateEvent]:
        if not self._started:
            self._metrics.start_processing()
            self._started = True
        
        start_time = time.time()
        events: List[StateEvent] = []
        
        if df_chunk.empty:
            return events
        
        if self._machine_id == "" and "machine_id" in df_chunk.columns:
            self._machine_id = str(df_chunk["machine_id"].iloc[0])
        
        for row in df_chunk.itertuples(index=False):
            timestamp = int(row.timestamp)
            
            if self._first_timestamp is None:
                self._first_timestamp = timestamp
            self._last_timestamp = timestamp
            
            if "cpu" in self._trackers and hasattr(row, "cpu_util"):
                event = self._process_point(
                    self._trackers["cpu"],
                    float(row.cpu_util),
                    timestamp
                )
                if event:
                    events.append(event)
            
            if "memory" in self._trackers and hasattr(row, "mem_util"):
                event = self._process_point(
                    self._trackers["memory"],
                    float(row.mem_util),
                    timestamp
                )
                if event:
                    events.append(event)
        
        duration = time.time() - start_time
        self._metrics.record_chunk(len(df_chunk), duration)
        
        self._events.extend(events)
        
        return events
    
    def _process_point(
        self, 
        tracker: MetricTracker, 
        value: float, 
        timestamp: int
    ) -> Optional[StateEvent]:
        tracker.values.append(value)
        tracker.timestamps.append(timestamp)
        tracker.means.append(tracker.baseline.mean)
        tracker.stds.append(tracker.baseline.std)
        
        deviation_result = tracker.deviation.update(value, timestamp)
        tracker.zscores.append(deviation_result.zscore)
        
        if not tracker.baseline.is_ready:
            return None
        
        persistence_result = tracker.persistence.check(deviation_result)
        
        transition = tracker.state_machine.process(
            persistence_result, 
            timestamp,
            zscore=deviation_result.zscore
        )
        
        if transition:
            self._metrics.record_transition(transition)
            return StateEvent(
                metric_name=tracker.name,
                transition=transition,
                machine_id=self._machine_id
            )
        
        return None
    
    @property
    def events(self) -> List[StateEvent]:
        return self._events.copy()
    
    @property
    def machine_id(self) -> str:
        return self._machine_id
    
    def get_tracker(self, metric: str) -> Optional[MetricTracker]:
        return self._trackers.get(metric)
    
    def get_transitions(self, metric: str) -> List[StateTransition]:
        tracker = self._trackers.get(metric)
        if tracker:
            return tracker.state_machine.transitions
        return []
    
    def get_current_state(self, metric: str) -> Optional[RegimeState]:
        tracker = self._trackers.get(metric)
        if tracker:
            return tracker.state_machine.current_state
        return None
    
    def get_time_series_data(self, metric: str) -> Dict[str, List]:
        tracker = self._trackers.get(metric)
        if not tracker:
            return {}
        
        return {
            "timestamps": tracker.timestamps.copy(),
            "values": tracker.values.copy(),
            "means": tracker.means.copy(),
            "stds": tracker.stds.copy(),
            "zscores": tracker.zscores.copy()
        }
    
    def get_all_metrics(self) -> Dict[str, Any]:
        total_duration = 0
        if self._first_timestamp and self._last_timestamp:
            total_duration = self._last_timestamp - self._first_timestamp
        
        result = {
            "machine_id": self._machine_id,
            "total_duration": total_duration,
            "systems": self._metrics.compute_systems_metrics().to_dict()
        }
        
        for name, tracker in self._trackers.items():
            transitions = tracker.state_machine.transitions
            result[name] = {
                "detection": self._metrics.compute_detection_quality(transitions).to_dict(),
                "stability": self._metrics.compute_stability(transitions, total_duration).to_dict(),
                "current_state": tracker.state_machine.current_state.value,
                "transition_count": len(transitions)
            }
        
        return result
    
    def stop(self) -> None:
        self._metrics.stop_tracking()
    
    def reset(self) -> None:
        for tracker in self._trackers.values():
            tracker.baseline.reset()
            tracker.deviation.reset()
            tracker.persistence.reset()
            tracker.state_machine.reset()
            tracker.values.clear()
            tracker.timestamps.clear()
            tracker.zscores.clear()
            tracker.means.clear()
            tracker.stds.clear()
        
        self._metrics.reset()
        self._events.clear()
        self._machine_id = ""
        self._first_timestamp = None
        self._last_timestamp = None
        self._started = False
    
    def __repr__(self) -> str:
        trackers = ", ".join(self._trackers.keys())
        return f"BlackicePipeline(trackers=[{trackers}], events={len(self._events)})"


def stream_machine_data(
    filepath: str,
    machine_id: str,
    chunksize: int = 500000,
    columns: Optional[List[str]] = None
) -> Iterator[pd.DataFrame]:
    if columns is None:
        columns = ["machine_id", "timestamp", "cpu_util", "mem_util", "c5", "c6", "c7", "c8", "c9"]
    
    reader = pd.read_csv(
        filepath,
        names=columns,
        header=None,
        usecols=["machine_id", "timestamp", "cpu_util", "mem_util"],
        chunksize=chunksize
    )
    
    for chunk in reader:
        filtered = chunk[chunk["machine_id"] == machine_id]
        if not filtered.empty:
            yield filtered.sort_values("timestamp")


# --- FILE: src/blackice/state.py ---

from dataclasses import dataclass
from enum import Enum
from typing import List, Optional

from .deviation import DeviationDirection
from .persistence import PersistenceResult, PersistenceStatus


class RegimeState(Enum):
    NORMAL = "NORMAL"
    UNSTABLE = "UNSTABLE"
    SHIFTED = "SHIFTED"


@dataclass
class StateTransition:
    from_state: RegimeState
    to_state: RegimeState
    timestamp: int
    direction: DeviationDirection
    reason: str
    zscore: float = 0.0
    
    def to_dict(self) -> dict:
        return {
            "from_state": self.from_state.value,
            "to_state": self.to_state.value,
            "timestamp": self.timestamp,
            "direction": self.direction.value,
            "reason": self.reason,
            "zscore": self.zscore
        }


@dataclass
class StateEvent:
    metric_name: str
    transition: StateTransition
    machine_id: str = ""
    
    def to_dict(self) -> dict:
        return {
            "metric_name": self.metric_name,
            "machine_id": self.machine_id,
            **self.transition.to_dict()
        }


class RegimeStateMachine:
    
    def __init__(self, metric_name: str = "metric"):
        self.metric_name = metric_name
        self._state: RegimeState = RegimeState.NORMAL
        self._transitions: List[StateTransition] = []
        self._unstable_since: Optional[int] = None
        self._shifted_since: Optional[int] = None
        self._last_direction: DeviationDirection = DeviationDirection.NONE
    
    def process(
        self, 
        persistence: PersistenceResult, 
        timestamp: int,
        zscore: float = 0.0
    ) -> Optional[StateTransition]:
        transition = None
        
        if self._state == RegimeState.NORMAL:
            if persistence.status == PersistenceStatus.WATCHING:
                transition = self._transition_to(
                    RegimeState.UNSTABLE,
                    timestamp,
                    persistence.direction,
                    zscore,
                    f"Significant deviation detected ({persistence.direction.value}), "
                    f"watching for persistence ({persistence.consecutive_count}/{persistence.required_count})"
                )
                self._unstable_since = timestamp
                
            elif persistence.status == PersistenceStatus.CONFIRMED:
                transition = self._transition_to(
                    RegimeState.SHIFTED,
                    timestamp,
                    persistence.direction,
                    zscore,
                    f"Immediate regime shift confirmed ({persistence.direction.value}), "
                    f"deviation persisted for {persistence.consecutive_count} points"
                )
                self._shifted_since = timestamp
        
        elif self._state == RegimeState.UNSTABLE:
            if persistence.status == PersistenceStatus.NOT_DEVIATING:
                transition = self._transition_to(
                    RegimeState.NORMAL,
                    timestamp,
                    DeviationDirection.NONE,
                    zscore,
                    f"Deviation did not persist (noise filtered), "
                    f"returning to normal after {timestamp - (self._unstable_since or 0)} time units"
                )
                self._unstable_since = None
                
            elif persistence.status == PersistenceStatus.CONFIRMED:
                transition = self._transition_to(
                    RegimeState.SHIFTED,
                    timestamp,
                    persistence.direction,
                    zscore,
                    f"Regime shift confirmed ({persistence.direction.value}), "
                    f"deviation persisted for {persistence.consecutive_count} points"
                )
                self._shifted_since = timestamp
                self._unstable_since = None
        
        elif self._state == RegimeState.SHIFTED:
            if persistence.status == PersistenceStatus.NOT_DEVIATING:
                transition = self._transition_to(
                    RegimeState.NORMAL,
                    timestamp,
                    DeviationDirection.NONE,
                    zscore,
                    f"System returned to baseline, "
                    f"regime lasted {timestamp - (self._shifted_since or 0)} time units"
                )
                self._shifted_since = None
                
            elif persistence.status == PersistenceStatus.WATCHING:
                if persistence.direction != self._last_direction:
                    transition = self._transition_to(
                        RegimeState.UNSTABLE,
                        timestamp,
                        persistence.direction,
                        zscore,
                        f"New deviation in opposite direction ({persistence.direction.value}), "
                        f"watching for new regime"
                    )
                    self._unstable_since = timestamp
                    self._shifted_since = None
        
        if persistence.status != PersistenceStatus.NOT_DEVIATING:
            self._last_direction = persistence.direction
        
        return transition
    
    def _transition_to(
        self, 
        new_state: RegimeState, 
        timestamp: int,
        direction: DeviationDirection,
        zscore: float,
        reason: str
    ) -> StateTransition:
        transition = StateTransition(
            from_state=self._state,
            to_state=new_state,
            timestamp=timestamp,
            direction=direction,
            zscore=zscore,
            reason=reason
        )
        self._state = new_state
        self._transitions.append(transition)
        return transition
    
    @property
    def current_state(self) -> RegimeState:
        return self._state
    
    @property
    def transitions(self) -> List[StateTransition]:
        return self._transitions.copy()
    
    @property
    def transition_count(self) -> int:
        return len(self._transitions)
    
    def reset(self) -> None:
        self._state = RegimeState.NORMAL
        self._transitions.clear()
        self._unstable_since = None
        self._shifted_since = None
        self._last_direction = DeviationDirection.NONE
    
    def __repr__(self) -> str:
        return (
            f"RegimeStateMachine(metric={self.metric_name}, "
            f"state={self._state.value}, transitions={len(self._transitions)})"
        )


# --- FILE: train_model.py ---
#!/usr/bin/env python3
import sys
import argparse
import pandas as pd
import yaml
from pathlib import Path
from dataclasses import asdict

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from blackice.learning.optimizer import GridSearchOptimizer
from blackice.learning.objective import AnomalyInterval


def generate_heuristic_ground_truth(df: pd.DataFrame, cpu_threshold: float = 95.0) -> list[AnomalyInterval]:
    """
    Generates 'Ground Truth' labels based on a simple heuristic for demonstration.
    In a real scenario, this would load human-labeled data.
    
    Heuristic: Continuous regions where CPU > threshold are 'True Anomalies'.
    """
    labels = []
    in_anomaly = False
    start_time = None
    
    # Sort by time just in case
    df = df.sort_values('timestamp')
    
    for _, row in df.iterrows():
        is_high = row['cpu_util'] > cpu_threshold
        
        if is_high and not in_anomaly:
            in_anomaly = True
            start_time = row['timestamp']
        elif not is_high and in_anomaly:
            in_anomaly = False
            labels.append(AnomalyInterval(start_time, row['timestamp']))
            
    # Close pending
    if in_anomaly:
        labels.append(AnomalyInterval(start_time, df.iloc[-1]['timestamp']))
        
    return labels

def main():
    parser = argparse.ArgumentParser(description="Train BlackIce Params (Offline Learning)")
    parser.add_argument('data', help="Path to training CSV (machine_usage.csv)")
    parser.add_argument('--output', default='configs/learned_config.yaml', help="Output config file")
    args = parser.parse_args()
    
    data_path = Path(args.data)
    if not data_path.exists():
        print(f"Error: Data file {data_path} not found.")
        sys.exit(1)
        
    print(f"Loading training data from {data_path}...")
    # Load assuming standard schema, but handle only needed columns for speed
    try:
        df = pd.read_csv(data_path)
    except Exception as e:
        print(f"Error reading CSV: {e}")
        sys.exit(1)
        
    # 1. Generate Ground Truth (Simulated Labels)
    print("Generating heuristic ground truth (CPU > 95%)...")
    ground_truth = generate_heuristic_ground_truth(df)
    print(f"Found {len(ground_truth)} mock anomaly intervals.")
    
    if not ground_truth:
        print("Warning: No anomalies found in data. Training might be trivial.")
        
    # 2. Configure Search Space
    # We use a smaller grid for the demo to run fast
    param_grid = {
        "window_size": [10, 20, 50, 100],
        "zscore_threshold": [2.5, 3.0, 4.0, 5.0],
        "min_consecutive_points": [3, 5, 8]
    }
    
    # 3. Run Optimization
    optimizer = GridSearchOptimizer(param_grid)
    best_config = optimizer.train(df, ground_truth)
    
    # 4. Save Result
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        config_dict = asdict(best_config)
        yaml.dump(config_dict, f)
        
    print("\n✅ Training Complete.")
    print(f"Learned Configuration saved to {output_path}")
    print("You can now run the detector with:")
    print(f"  python src/blackice/cli.py --config {output_path} ...")

if __name__ == "__main__":
    main()
